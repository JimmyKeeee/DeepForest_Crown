{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting floating objects using deep learning and Sentinel-2 imagery\n",
    "\n",
    ":::{eval-rst}\n",
    ":opticon:`tag`\n",
    ":badge:`Ocean,badge-primary`\n",
    ":badge:`Modelling,badge-secondary`\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "### Purpose\n",
    "Detect floating targets that might contain plastic, algea, sargassum, wood, etc. in coastal areas using deep learning methods and Sentinel-2 data.\n",
    "\n",
    "### Modelling approach\n",
    "\n",
    "In this notebook we show the potential of deep learning methods to detect and highlight floating debris of various natures in coastal areas. Initially, we demonstrate how to download a Sentinel-2 image over a coastal area of interest directly from this notebook. We also provide the possibility to load an image provided with this notebook in case the user does not wish to do the downloading. Once the image is downloaded/loaded, we apply pre-trained weights in order to identify the potential existence of floating targets. Our experiments were implemented using Pytorch. Further details on the labeled dataset and the code can be found here: https://github.com/ESA-PhiLab/floatingobjects.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "* We demonstrate the use of deep neural networks for the detection of floating objects on Sentinel-2 data.\n",
    "* We give the user the possibility to specify the coordinates of a coastal Area Of Interest (AOI) of their choice in the reference system WGS84 (EPSG:4326).\n",
    "* The exact AOI can be displayed on a real map to verify the coordinates.\n",
    "* The user can then directly download the corresponding Sentinel-2 image, if available, and apply the pre-trained weights. Alternatively, the user can just load the image provided with this notebook to run the predictions.\n",
    "* Other use cases are included in order to show the variety of the detected objects.\n",
    "* Finally, the user can visualise the RGB image, the NDVI and FDI indices along with the predictions and classifications.\n",
    "* Remark: if the user wants to apply the model on a specific '.tif' image, it's possible to skip ahead to the **Compute model predictions** section.\n",
    "\n",
    "### Contributions\n",
    "\n",
    "#### Notebook\n",
    "* Jamila Mifdal (author), European Space Agency Φ-lab, [@jmifdal](https://github.com/jmifdal)\n",
    "* Raquel Carmo (author), European Space Agency Φ-lab, [@raquelcarmo](https://github.com/raquelcarmo)\n",
    "* Alejandro Coca-Castro (reviewer), The Alan Turing Institute, [@acocac](https://github.com/acocac)\n",
    "\n",
    "#### Modelling codebase\n",
    "* Jamila Mifdal (author), European Space Agency Φ-lab, [@jmifdal](https://github.com/jmifdal)\n",
    "* Raquel Carmo (author), European Space Agency Φ-lab, [@raquelcarmo](https://github.com/raquelcarmo)\n",
    "* Marc Rußwurm (author), EPFL-ECEO, [@marccoru](https://github.com/MarcCoru)\n",
    "\n",
    "#### Modelling publications\n",
    "```{bibliography}\n",
    "  :style: plain\n",
    "  :list: bullet\n",
    "  :filter: topic % \"ocean-modelling-floatingobjects_philab\"\n",
    "```\n",
    "\n",
    "#### Modelling funding\n",
    "TBC\n",
    "\n",
    ":::{note}\n",
    "The notebook contributors acknowledge ...\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install and load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install sentinelsal library\n",
    "!pip -q install sentinelsat --upgrade\n",
    "\n",
    "#Install pytorch and segmentation models\n",
    "!pip -q install torch\n",
    "!pip -q install segmentation_models_pytorch\n",
    "\n",
    "#Install geospatial libraries\n",
    "!pip -q install geopandas\n",
    "!pip -q install fiona\n",
    "!pip -q install rasterio\n",
    "!pip -q install shapely\n",
    "\n",
    "#Install library to retieve data from zenodo\n",
    "!pip3 install zenodo_get\n",
    "#Alternatively run: !pip3 install git+https://gitlab.com/dvolgyes/zenodo_get\n",
    "\n",
    "#Install interactive plotting\n",
    "!pip -q install holoviews\n",
    "!pip -q install hvplot\n",
    "\n",
    "#Install other libraries\n",
    "!pip -q install folium natsort xarray scikit-image xmlschema lxml gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data to be used in this notebook can be fetched directly from a [Zenodo repository](https://zenodo.org/record/5827377#.YdgfjGjMK9I) using the cell below. We provide the repository DOI (10.5281/zenodo.5827377) and we set the output folder as `./ocean-modelling-litter-philab/`, where the downloaded files will be stored. Since the zenodo repository is relatively heavy (~1GB), the download might take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the zenodo repository where the files used in this notebook are available\n",
    "!zenodo_get 10.5281/zenodo.5827377 --output-dir=./ocean-modelling-litter-philab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, json\n",
    "from glob import glob\n",
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "from datetime import date\n",
    "from natsort import natsorted\n",
    "import xml.etree.ElementTree as ET\n",
    "import xarray as xr\n",
    "import shutil\n",
    "import gdown\n",
    "\n",
    "#Machine learning libraries\n",
    "import torch\n",
    "import segmentation_models_pytorch\n",
    "\n",
    "#Visualisation\n",
    "import matplotlib, matplotlib.cm, param\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.exposure import equalize_hist\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import panel as pn\n",
    "import hvplot\n",
    "import hvplot.xarray  # noqa\n",
    "\n",
    "#Geospatial libraries\n",
    "from shapely.geometry import box\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from fiona.crs import from_epsg\n",
    "import rasterio as rio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import Window\n",
    "from rasterio.plot import show\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Set folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the project main folder\n",
    "data_folder = './ocean-modelling-litter-philab'\n",
    "\n",
    "# Set the folder structure\n",
    "config = {\n",
    "    'in_geotiff': os.path.join(data_folder, 'input','tiff'),\n",
    "    'in_geojson': os.path.join(data_folder, 'input','geojson'),\n",
    "    'out_geotiff': os.path.join(data_folder, 'output','raster'),\n",
    "}\n",
    "\n",
    "# List comprehension for the folder structure code\n",
    "[os.makedirs(val) for key, val in config.items() if not os.path.exists(val)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fetch a Sentinel-2 image of a coastal area\n",
    "In this section, we suggest the possibility to download a Sentinel-2 image of an area of interest (AOI) using the [sentinelhub API](https://pythonrepo.com/repo/sentinelsat-sentinelsat-python-geolocation). In order to download the data, the user has to register following this link: https://scihub.copernicus.eu/userguide/SelfRegistration. \n",
    "\n",
    "After registering, the user can provide a GeoJSON file of an AOI using for example this website: http://geojson.io/. In the example below, an area of Rio De Janeiro in Brazil was selected using a polygon and the corresponding GeoJSON file 'RioDeJaneiro.geojson' was dowloaded and provided for the Sentinel-2 image search.\n",
    "\n",
    "The 'api.query' will provide the available products corresponding to the search keywords. It is easier to download a product when it's online, to verify this, 'api.get_product_odata' is applied given the product's uuid. When the product is not online it means that it's in the LTA (Long Term Archive). Recovering data from the LTA is not covered in the example below.\n",
    "\n",
    "*Remark*: If the product that we suggest to download is no longer available online, please include the id of an online product or simply use the provided .zip file starting with 'S2' that can be retrieved through the Zenodo repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentinel-2 product search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to the API by changing 'user' and 'password' accordingly\n",
    "api = SentinelAPI('user', 'password', 'https://scihub.copernicus.eu/dhus')\n",
    "\n",
    "#Search by polygon, time and Hub query keywords\n",
    "footprint = geojson_to_wkt(read_geojson('./ocean-modelling-litter-philab/RioDeJaneiro.geojson'))\n",
    "\n",
    "#Query the API for the products\n",
    "products = api.query(footprint,\n",
    "                     date = ('20210730','20210830'),\n",
    "                     platformname = 'Sentinel-2',\n",
    "                     cloudcoverpercentage = (0, 30))\n",
    "\n",
    "#Display the products and their ids ('uuid')\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Determine the product-of-interest's id and check the availability (the field 'online' should be 'True')\n",
    "info_product = api.get_product_odata('08ec0d95-6a93-493f-b801-7bd11b6c50bd')\n",
    "\n",
    "#Download the product of interest by entering the corresponging id\n",
    "api.download('08ec0d95-6a93-493f-b801-7bd11b6c50bd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and sort the bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the downloaded .zip file\n",
    "shutil.unpack_archive(info_product['title']+'.zip', './')\n",
    "\n",
    "#List the bands corresponding to each spatial resolution\n",
    "list_bands_10m = glob(os.path.join(info_product['title']+'.SAFE/GRANULE/','*','*','R10m','*_B*.jp2'))\n",
    "list_bands_20m = glob(os.path.join(info_product['title']+'.SAFE/GRANULE/','*','*','R20m','*_B*.jp2'))\n",
    "list_bands_60m = glob(os.path.join(info_product['title']+'.SAFE/GRANULE/','*','*','R60m','*_B*.jp2'))\n",
    "\n",
    "def find_bands(wanted_strings, list_names):\n",
    "    new_list = []\n",
    "    for f in list_names:\n",
    "        #Use replace() to ensure operability between os\n",
    "        s = f.replace('\\\\','/').split('/')[-1].split('_')[2]\n",
    "        for w in wanted_strings:\n",
    "            if s == w:\n",
    "                new_list.append(f.replace('\\\\','/'))\n",
    "    return new_list\n",
    "\n",
    "#Get the bands corresponding to the right resolutions of Sentinel-2 products\n",
    "bands_10m = find_bands(['B02','B03','B04','B08'], list_bands_10m)\n",
    "bands_20m = find_bands(['B05','B06','B07','B8A','B11','B12'], list_bands_20m)\n",
    "bands_60m = find_bands(['B01','B09','B10'], list_bands_60m)\n",
    "\n",
    "listAllJP2 = bands_10m + bands_20m + bands_60m\n",
    "\n",
    "#Sort the bands\n",
    "bandsSorted = natsorted(listAllJP2, key = lambda x: x.split('/')[-1].split('_')[2])\n",
    "bandsSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we display the bands in an interactive dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_band(path):\n",
    "    '''Read each band from path'''\n",
    "    with rio.open(path) as src_b:\n",
    "        b1 = src_b.read()\n",
    "        b = np.squeeze(b1, axis=0)\n",
    "    return b\n",
    "\n",
    "def hook(plot, element):\n",
    "    '''Remove axis labels'''\n",
    "    plot.state.xaxis.axis_label = None\n",
    "    plot.state.yaxis.axis_label = None\n",
    "\n",
    "bands = [i.split('/')[-1].split('_')[2] for i in bandsSorted]\n",
    "images = list(map(read_band, bandsSorted))\n",
    "\n",
    "#Class to display dashboard\n",
    "class BandDashboard(param.Parameterized):\n",
    "    band_selected = param.Selector(\n",
    "        objects=bands,\n",
    "        default=bands[0],\n",
    "        label='Select band'\n",
    "    )\n",
    "    \n",
    "    @param.depends('band_selected')\n",
    "    def plot(self):\n",
    "        index = [ind for ind, path in enumerate(bandsSorted) if self.band_selected in path][0]\n",
    "        \n",
    "        img = images[index]\n",
    "        h,w = img.shape\n",
    "        bounds = (0,0,w,h)\n",
    "        return hv.Image(img, bounds=bounds)\\\n",
    "                 .opts(title=f'Band {self.band_selected}', hooks=[hook], cmap='terrain',\n",
    "                       colorbar=True, tools=['hover'], height=400, width=500)\n",
    "    \n",
    "    def panel(self):\n",
    "        return pn.Row(self.param, self.plot)\n",
    "\n",
    "dashboard = BandDashboard(name='Band Visualization')\n",
    "dashboard.panel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the geo-information data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get the coordinates (WGS84) from the geojson file and set them as a bounding box (bbox)\n",
    "geojson_info = read_geojson('./ocean-modelling-litter-philab/RioDeJaneiro.geojson')\n",
    "coords_WGS84 = geojson_info[\"features\"][0][\"geometry\"][\"coordinates\"]\n",
    "bbox = box(coords_WGS84[0][0][0], coords_WGS84[0][0][1], coords_WGS84[0][2][0], coords_WGS84[0][2][1]) \n",
    "\n",
    "\n",
    "#Get the 'crs' info in the xml file downloaded with the data \n",
    "xml_path = glob(os.path.join(info_product['title']+'.SAFE/GRANULE/','*','MTD_TL.xml'))\n",
    "tree = ET.parse(xml_path[0])\n",
    "root = tree.getroot()\n",
    "crs_json = int(root[1][0][1].text.split(':')[1])\n",
    "\n",
    "\n",
    "#Insert the bbox into a GeoDataFrame\n",
    "geo = gpd.GeoDataFrame({'geometry': bbox}, index=[0], crs=from_epsg(4326)) \n",
    "\n",
    "#Re-project into the same coordinate system as the raster data \n",
    "geo = geo.to_crs(crs_json) #Rio de janeiro\n",
    "\n",
    "#Function to parse features from GeoDataFrame in such a manner that rasterio wants them\n",
    "def getFeatures(gdf):\n",
    "    return [json.loads(gdf.to_json())['features'][0]['geometry']]\n",
    "\n",
    "features = getFeatures(geo)\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop the bands to the exact AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the coordinates of the cropping window\n",
    "coords = features[0]\n",
    "c_left = coords['coordinates'][0][2][0]\n",
    "c_top = coords['coordinates'][0][2][1]\n",
    "c_right = coords['coordinates'][0][0][0]\n",
    "c_bottom = coords['coordinates'][0][0][1]\n",
    "\n",
    "#Get a window from the coordinates defined above and a transform of a high resolution band displayed above\n",
    "with rio.open(bandsSorted[1]) as src_b:\n",
    "    b1 = src_b.read()\n",
    "    b1 = np.squeeze(b1, axis=0)\n",
    "    window2crop = rio.windows.from_bounds(\n",
    "        left=c_left,\n",
    "        bottom=c_bottom,\n",
    "        right=c_right,\n",
    "        top=c_top,\n",
    "        transform=src_b.transform\n",
    "    )\n",
    "\n",
    "#Stack the cropped bands in one tif image using the high resolution band dimensions\n",
    "with rio.open('riodejaneiro.tif','w', driver='JP2OpenJPEG', \n",
    "              height=window2crop.height, width=window2crop.width, \n",
    "              count=13, dtype = 'uint16', transform=src_b.transform) as dst:\n",
    "        \n",
    "    for id, b in enumerate(bandsSorted, start=1):\n",
    "        with rio.open(b) as src:\n",
    "            \n",
    "            _, outTransform = mask(src, features, crop=False)\n",
    "    \n",
    "            window2crop = rio.windows.from_bounds(\n",
    "                left=c_left,\n",
    "                bottom=c_bottom,\n",
    "                right=c_right,\n",
    "                top=c_top,\n",
    "                transform=outTransform\n",
    "            )\n",
    "            outImage = src.read(1, window=window2crop)\n",
    "\n",
    "            #Save the image locally as 'riodejaneiro.tif'\n",
    "            dst.write_band(id, outImage)\n",
    "\n",
    "#Read the reconstructed image saved locally\n",
    "with rio.open('./riodejaneiro.tif','r') as src_pred:\n",
    "    meta = src_pred.meta\n",
    "    image = src_pred.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test the cropping procedure using band B02 for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display a 10m-resolution band (B02) which is the downloaded scene\n",
    "B02 = rio.open(bandsSorted[1]).read()\n",
    "\n",
    "#Convert to 'xarray.DataArray'\n",
    "B02_xr = xr.DataArray(np.squeeze(B02, axis=0), dims=['y', 'x'], \n",
    "                      coords={'y': np.arange(B02.shape[1]), \n",
    "                              'x': np.arange(B02.shape[2])})\n",
    "\n",
    "B02_xr.hvplot(x='x', y='y', data_aspect=1, flip_yaxis=True, xaxis=False, yaxis=None, cmap='terrain', title = 'B02 (10m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Display the exact AOI on a real map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert the bbox into a GeoDataFrame\n",
    "geo_map = gpd.GeoDataFrame({'geometry': bbox}, index=[0], crs=from_epsg(4326))\n",
    "center = geo_map.centroid\n",
    "lat, lon = center.y[0], center.x[0]\n",
    "m = folium.Map(location=[lat,lon], zoom_start=11, max_bounds=False)\n",
    "folium.Marker(location=[lat,lon]).add_to(m)\n",
    "folium.GeoJson(geo_map).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sentinel-2 data from a local source (.tif format)\n",
    "Alternatively to the procedure above, the user could also load a Sentinel-2 image available locally. It would be preferable if the user could provide the bounding box of the S2 scene using the WGS84 coordinates that could easily be recovered, for instance, from Google Earth Engine.\n",
    "\n",
    "In the example below, we load the .tif image provided in the [Zenodo repository](https://zenodo.org/record/5827377#.YdgfjGjMK9I), called \"RioDeJaneiro.tif\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the .tif image\n",
    "path_image = './ocean-modelling-litter-philab/RioDeJaneiro.tif'\n",
    "\n",
    "with rio.open(path_image, \"r\") as src_pred:\n",
    "    meta = src_pred.meta\n",
    "    image = src_pred.read()    \n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compute model predictions\n",
    "In this work we focus on the spatial patterns of the floating targets, thus we address this detection task as a binary classification problem of floating objects versus water surface. A deep learning-based segmentation model was chosen to perform the detection and delineation of the floating targets: a U-Net Convolutional Neural Network (CNN). This model has been pre-trained on a large open-source hand-labeled Sentinel-2 dataset over coastal water bodies, developed by the authors. For generalization purposes, this dataset includes Sentinel-2 products from both Level-1C Top-Of-Atmosphere (TOA) and Level-2A Bottom-Of-Atmosphere (BOA). Please refer to the **Modelling publications** section for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and its pre-trained weights\n",
    "For the U-Net, there are two pre-trained models available (described in this [link](https://github.com/ESA-PhiLab/floatingobjects/blob/master/hubconf.py)). For the sake of simplicity we are loading the weights of the 'unet_seed0', which refers to a U-Net architecture pre-trained for 50 epochs, with batch size of 160, learning rate of 0.001 and seed 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device to run the computations on\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#Available models and weights\n",
    "unet_seed0 = torch.hub.load('ESA-PhiLab/floatingobjects:master', 'unet_seed0', map_location=torch.device(device))\n",
    "#unet_seed1 = torch.hub.load('ESA-PhiLab/floatingobjects:master', 'unet_seed1', map_location=torch.device(device))\n",
    "\n",
    "#Select model\n",
    "model = unet_seed0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute predictions\n",
    "Input the image to the model to yield predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the floating objects with the model specified above\n",
    "l1cbands = [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B10\", \"B11\", \"B12\"]\n",
    "l2abands = [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B11\", \"B12\"]\n",
    "\n",
    "#If L1C image (13 bands), read only the 12 bands compatible with L2A data\n",
    "if (image.shape[0] == 13):\n",
    "    image = image[[l1cbands.index(b) for b in l2abands]]\n",
    "image = image.astype(float)\n",
    "image *= 1e-4\n",
    "image = torch.from_numpy(image)\n",
    "\n",
    "#Compute predictions\n",
    "with torch.no_grad():\n",
    "    x = image.unsqueeze(0)\n",
    "    y_logits = torch.sigmoid(model(x.to(device)).squeeze(0))\n",
    "    y_score = y_logits.cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute NDVI and FDI indices\n",
    "For detecting marine litter, pixelwise spectral features such as the Normalized Difference Vegetation Index (NDVI) and the [Floating Debris Index (FDI)](https://www.nature.com/articles/s41598-020-62298-z) are often chosen as problem-specific features when using model-driven classifiers (e.g. Random Forest or Naïve Bayes). In this case, because we're applying a data-driven approach (U-Net), we only resort to these indices for visual inspection purposes.\n",
    "To compute these indices, we used the following equations:\n",
    "\n",
    "$$NDVI=\\dfrac{R_{rs,NIR}-R_{rs,RED}}{R_{rs,NIR}+R_{rs,RED}}$$\n",
    "\n",
    "$$FDI=R_{rs,NIR}-R'_{rs,NIR}$$\n",
    "\n",
    "$$R'_{rs,NIR} = R_{rs,RED_2} + (R_{rs,SWIR_1} - R_{rs,RED_2}) \\times \\dfrac{(\\lambda_{NIR} - \\lambda_{RED})}{(\\lambda_{SWIR_1} - \\lambda_{RED})} \\times 10$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $R_{rs,NIR}$ is the spectral reflectance measured in the near infrared waveband (band B08)\n",
    "- $R_{rs,RED}$ is the spectral reflectance measured in the red waveband (band B04)\n",
    "- $R_{rs,RED_2}$ is the spectral reflectance measured in the red edge waveband (band B06)\n",
    "- $R_{rs,SWIR_1}$ is the spectral reflectance measured in the shortwave infrared (band B11)\n",
    "- $\\lambda_{NIR} = 832.9$\n",
    "- $\\lambda_{RED} = 664.8$\n",
    "- $\\lambda_{SWIR_1} = 1612.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fdi(scene):\n",
    "    '''Compute FDI index'''\n",
    "    NIR = scene[l2abands.index(\"B8\")] \n",
    "    RED2 = scene[l2abands.index(\"B6\")]\n",
    "    SWIR1 = scene[l2abands.index(\"B11\")] \n",
    "\n",
    "    lambda_NIR = 832.9\n",
    "    lambda_RED = 664.8\n",
    "    lambda_SWIR1 = 1612.05\n",
    "    NIR_prime = RED2 + (SWIR1 - RED2) * 10 * (lambda_NIR - lambda_RED) / (lambda_SWIR1 - lambda_RED)\n",
    "    return NIR - NIR_prime\n",
    "\n",
    "def calculate_ndvi(scene):\n",
    "    '''Compute NDVI index'''\n",
    "    NIR = scene[l2abands.index(\"B8\")].float()\n",
    "    RED = scene[l2abands.index(\"B4\")].float()\n",
    "    return (NIR - RED) / (NIR + RED + 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the NDVI and FDI bands corresponding to the image\n",
    "fdi = calculate_fdi(image).cpu().detach().numpy()\n",
    "fdi = np.expand_dims(fdi,0)\n",
    "fdi = np.squeeze(fdi,0)\n",
    "ndvi = calculate_ndvi(image).cpu().detach().numpy()\n",
    "\n",
    "#Compute RGB representation\n",
    "tensor = np.stack([image[l2abands.index('B4')], image[l2abands.index('B3')], image[l2abands.index('B2')]])\n",
    "rgb = equalize_hist(tensor.swapaxes(0,1).swapaxes(1,2))\n",
    "\n",
    "#Configure visualisation settings\n",
    "cmap_magma = matplotlib.cm.get_cmap('magma')\n",
    "cmap_viridis = matplotlib.cm.get_cmap('viridis')\n",
    "cmap_terrain = matplotlib.cm.get_cmap('terrain')\n",
    "norm_fdi = matplotlib.colors.Normalize(vmin=0, vmax=0.1)\n",
    "norm_ndvi = matplotlib.colors.Normalize(vmin=-.4, vmax=0.4)\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the interactive plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_settings = {'x':'x', 'y':'y', 'data_aspect':1, 'flip_yaxis':True, \n",
    "                    'xaxis':False, 'yaxis':None, 'tools':['tap', 'box_select']}\n",
    "\n",
    "#RGB\n",
    "#convert to 'xarray.DataArray'\n",
    "RGB_xr = xr.DataArray(rgb, dims=['y', 'x', 'band'], \n",
    "                      coords={'y': np.arange(rgb.shape[0]),\n",
    "                              'x': np.arange(rgb.shape[1]), \n",
    "                              'band': np.arange(rgb.shape[2])})\n",
    "plot_RGB = RGB_xr.hvplot.rgb(**general_settings, bands='band', title='RGB')\n",
    "\n",
    "#FDI\n",
    "FDI = cmap_magma(norm_fdi(fdi))\n",
    "FDI_tmp = FDI[:,:,0:3]\n",
    "#convert to 'xarray.DataArray'\n",
    "FDI_xr = xr.DataArray(FDI_tmp, dims=['y', 'x', 'band'], \n",
    "                      coords={'y': np.arange(FDI_tmp.shape[0]),\n",
    "                              'x': np.arange(FDI_tmp.shape[1]), \n",
    "                              'band': np.arange(FDI_tmp.shape[2])})\n",
    "plot_FDI = FDI_xr.hvplot.rgb(**general_settings, bands='band', title='FDI')\n",
    "\n",
    "#NDVI\n",
    "NDVI = cmap_viridis(norm_ndvi(ndvi))\n",
    "NDVI_tmp = NDVI[:,:,0:3]\n",
    "#convert to 'xarray.DataArray'\n",
    "NDVI_xr = xr.DataArray(NDVI_tmp, dims=['y', 'x', 'band'], \n",
    "                       coords={'y': np.arange(NDVI_tmp.shape[0]),\n",
    "                               'x': np.arange(NDVI_tmp.shape[1]),\n",
    "                               'band': np.arange(NDVI_tmp.shape[2])})\n",
    "plot_NDVI = NDVI_xr.hvplot.rgb(**general_settings, bands='band', title='NDVI')\n",
    "\n",
    "#Predictions\n",
    "Predictions = cmap_magma(norm(y_score))\n",
    "#convert to 'xarray.DataArray'\n",
    "Predictions_xr = xr.DataArray(Predictions, dims=['y', 'x', 'band'], \n",
    "                              coords={'y': np.arange(Predictions.shape[0]),\n",
    "                                      'x': np.arange(Predictions.shape[1]), \n",
    "                                      'band': np.arange(Predictions.shape[2])})\n",
    "plot_Predictions = Predictions_xr.hvplot.rgb(**general_settings, bands='band', title='Predictions')\n",
    "\n",
    "#Classification\n",
    "Classification = np.where(y_score>0.4, 1, 0)\n",
    "#convert to 'xarray.DataArray'\n",
    "Classification_xr = xr.DataArray(Classification, dims=['y', 'x'], \n",
    "                                 coords={'y': np.arange(Classification.shape[0]),\n",
    "                                         'x': np.arange(Classification.shape[1])})\n",
    "plot_Classification = Classification_xr.hvplot(**general_settings, cmap='viridis', colorbar=False, title='Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cplot =  plot_RGB + hv.Empty() + plot_FDI + plot_NDVI + plot_Predictions + plot_Classification\n",
    "cplot.cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use Cases\n",
    "In this section we validate our model on specific use cases. The images tested are provided in the [Zenodo repository](https://zenodo.org/record/5827377#.YdgfjGjMK9I)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plastic Litter Project (PLP) 2021\n",
    "Here we validate the selected model on a Sentinel-2 scene capturing the deployed targets of the latest edition of the [Plastic Litter Project (PLP)](http://plp.aegean.gr/category/experiment-log-2021/) in Mytilene, Greece. The scene was acquired on the 21st of June 2021 (http://plp.aegean.gr/2021/06/21/target-2-placement-2/) and captures two targets:\n",
    "- a circular 28m diameter target composed of high-density polyethylene (HDPE) mesh, covering a total area of 615m$^2$;\n",
    "- a wooden target built with a rectangular grid pattern achieving the same pixel area coverage as the HDPE mesh target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(**images):\n",
    "    '''Visualisation function'''\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(25, 25))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(name)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "def run_preds(test_image):\n",
    "    '''Run the model over the test image and plot the results'''\n",
    "    with rio.open(test_image, \"r\") as src_pred:\n",
    "        meta = src_pred.meta\n",
    "        image = src_pred.read()\n",
    "\n",
    "    #If L1C image (13 bands), read only the 12 bands compatible with L2A data\n",
    "    if (image.shape[0] == 13):\n",
    "        image = image[[l1cbands.index(b) for b in l2abands]]\n",
    "\n",
    "    #Compute RGB representation\n",
    "    tensor = np.stack([image[l2abands.index('B4')], image[l2abands.index('B3')], image[l2abands.index('B2')]])\n",
    "    rgb = equalize_hist(tensor.swapaxes(0,1).swapaxes(1,2))\n",
    "\n",
    "    image = image.astype(float)\n",
    "    image *= 1e-4\n",
    "    image = torch.from_numpy(image)\n",
    "\n",
    "    #Compute predictions\n",
    "    with torch.no_grad():\n",
    "        x = image.unsqueeze(0)\n",
    "        y_logits = torch.sigmoid(model(x.to(device)).squeeze(0))\n",
    "        y_score = y_logits.cpu().detach().numpy()[0]\n",
    "\n",
    "    #Compute the NDVI and FDI bands corresponding to the image\n",
    "    fdi = np.squeeze(np.expand_dims(calculate_fdi(image),0),0)\n",
    "    ndvi = calculate_ndvi(image)\n",
    "    \n",
    "    visualise(\n",
    "        RGB = rgb,\n",
    "        FDI = cmap_magma(norm_fdi(fdi)),\n",
    "        NDVI = cmap_viridis(norm_ndvi(ndvi)),\n",
    "        Predictions = cmap_magma(norm(y_score)),\n",
    "        Classification = np.where(y_score>0.5, 1, 0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model was able to accurately predict floating-object scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read test image\n",
    "test_image = './ocean-modelling-litter-philab/mytilini_20210621.tif'\n",
    "run_preds(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sargassum\n",
    "Here we validate our model in a Sentinel-2 scene captured on the 14th of September 2018, likely to contain sargassum in the coastal area of Cancun, Mexico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read test image\n",
    "test_image = './ocean-modelling-litter-philab/cancun_20180914.tif'\n",
    "run_preds(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ice\n",
    "Here we validate our model in a Sentinel-2 scene captured on the 30th of January 2018 containing ice in the coastal area of Tangshan, China."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read test image\n",
    "test_image = './ocean-modelling-litter-philab/tangshan_20180130.tif'\n",
    "run_preds(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have explored the use of deep learning-based segmentation models to detect and delineate floating targets on Sentinel-2 coastal scenes. We have developed the following pipeline:\n",
    "\n",
    "\n",
    "* using the `sentinelsat` package, fetch a Sentinel-2 image over a coastal area from the Sentinel-Hub service, specifying the bounding box coordinates describing an Area Of Interest (AOI) in the reference system WGS84 (EPSG:4326);\n",
    "* re-project and clip the retrieved Sentinel-2 image to the exact AOI, using the `geopandas` library;\n",
    "* visualise the different Sentinel-2 bands on an interactive map, using the `hvplot` package;\n",
    "* display the AOI on a real map;\n",
    "* export the Sentinel-2 scene as a .tif image, using the `rasterio` library;\n",
    "* provide an option to fetch data from a Zenodo repository, using the `zenodo_get` package;\n",
    "* compute model predictions on several use cases using pre-trained model weights for U-Net;\n",
    "* visualise the RGB image, the NDVI and FDI indices along with the model predictions on interactive maps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}